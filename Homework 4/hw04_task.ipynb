{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайные леса\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
    "\n",
    "__Тема письма: `[ML][HW04] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n",
    "\n",
    "В этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным пользователей социальной сети Вконтакте, и сравнить его эффективность с ансамблем, предоставляемым библиотекой CatBoost.\n",
    "\n",
    "В результате мы сможем определить, какие подписки пользователей больше всего влияют на определение возраста и пола человека. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy\n",
    "from collections import Counter \n",
    "from catboost import Pool, CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(x):\n",
    "    _, counts = np.unique(x, return_counts=True)\n",
    "    proba = counts / len(x)\n",
    "    return np.sum(proba * (1 - proba))\n",
    "    \n",
    "def entropy(x):\n",
    "    _, counts = np.unique(x, return_counts=True)\n",
    "    proba = counts / len(x)\n",
    "    return -np.sum(proba * np.log2(proba))\n",
    "\n",
    "def gain(left_y, right_y, criterion):\n",
    "    y = np.concatenate((left_y, right_y))\n",
    "    return criterion(y) - (criterion(left_y) * len(left_y) + criterion(right_y) * len(right_y)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 (2 балла)\n",
    "Random Forest состоит из деревьев решений. Каждое такое дерево строится на одной из выборок, полученных при помощи bagging. Элементы, которые не вошли в новую обучающую выборку, образуют out-of-bag выборку. Кроме того, в каждом узле дерева мы случайным образом выбираем набор из `max_features` и ищем признак для предиката разбиения только в этом наборе.\n",
    "\n",
    "Сегодня мы будем работать только с бинарными признаками, поэтому нет необходимости выбирать значение признака для разбиения.\n",
    "\n",
    "#### Методы\n",
    "`predict(X)` - возвращает предсказанные метки для элементов выборки `X`\n",
    "\n",
    "#### Параметры конструктора\n",
    "`X, y` - обучающая выборка и соответствующие ей метки классов. Из нее нужно получить выборку для построения дерева при помощи bagging. Out-of-bag выборку нужно запомнить, она понадобится потом.\n",
    "\n",
    "`criterion=\"gini\"` - задает критерий, который будет использоваться при построении дерева. Возможные значения: `\"gini\"`, `\"entropy\"`.\n",
    "\n",
    "`max_depth=None` - ограничение глубины дерева. Если `None` - глубина не ограничена\n",
    "\n",
    "`min_samples_leaf=1` - минимальное количество элементов в каждом листе дерева.\n",
    "\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, split_dim, split_value, left, right):\n",
    "        self.split_dim = split_dim\n",
    "        self.split_value = split_value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "class DecisionTreeLeaf:\n",
    "    def __init__(self, probs, size):\n",
    "        self.probs = probs\n",
    "        self.size = size\n",
    "        self.y = max(probs, key=probs.get)`max_features=\"auto\"` - количество признаков, которые могут использоваться в узле. Если `\"auto\"` - равно `sqrt(X.shape[1])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(X, y):\n",
    "    Xb = np.zeros(X.shape)\n",
    "    yb = np.zeros(y.shape, dtype=y.dtype)\n",
    "    ind = set(range(X.shape[0]))\n",
    "    for i in range(X.shape[0]):\n",
    "        j = np.random.randint(X.shape[0])\n",
    "        Xb[i] = X[j]\n",
    "        yb[i] = y[j]\n",
    "        ind.discard(j)\n",
    "    ind = list(ind)\n",
    "    Xoob = np.zeros(shape=(len(ind),X.shape[1]))\n",
    "    yoob = np.zeros(len(ind), dtype=y.dtype)\n",
    "    for i in range(len(ind)):\n",
    "        Xoob[i] = X[ind[i]]\n",
    "        yoob[i] = y[ind[i]]\n",
    "    return Xb, yb, Xoob, yoob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    def __init__(self, split_dim, split_value, left, right):\n",
    "        self.split_dim = split_dim\n",
    "        self.split_value = split_value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "class DecisionTreeLeaf:\n",
    "    def __init__(self, probs, size):\n",
    "        self.probs = probs\n",
    "        self.size = size\n",
    "        self.y = max(probs, key=probs.get)\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, X, y, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\"):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        \n",
    "        if max_features == \"auto\":\n",
    "            self.max_features = int(np.sqrt(X.shape[1]))\n",
    "        else:\n",
    "            self.max_features = max_features\n",
    "        if criterion == \"gini\":\n",
    "            self.criterion = gini\n",
    "        elif criterion == \"entropy\":\n",
    "            self.criterion = entropy\n",
    "        else:\n",
    "            raise ValueError(f\"wrong criterion {criterion}\")\n",
    "            \n",
    "        self.X, self.y, self.Xoob, self.yoob = bagging(X, y)\n",
    "        self.fit(self.X, self.y)\n",
    "            \n",
    "    def make_node(self, ind, X, y, depth):\n",
    "        def make_leaf():\n",
    "            uni = np.unique(y)\n",
    "            probs = dict(zip(uni, np.zeros(len(uni))))\n",
    "            cur_uni, cnt = np.unique(y[ind], return_counts=True)\n",
    "            probs2 = dict(zip(cur_uni, cnt/len(ind)))\n",
    "            probs.update(probs2)\n",
    "            return DecisionTreeLeaf(probs, len(ind))\n",
    "        \n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or len(ind) < 2*self.min_samples_leaf or len(np.unique(y[ind])) == 1:\n",
    "            return make_leaf()\n",
    "        else:\n",
    "            best_f = None\n",
    "            best_s = 0.5\n",
    "            best_left = None\n",
    "            best_right = None\n",
    "            best_gain = None\n",
    "\n",
    "            for k in np.random.choice(range(X.shape[1]), self.max_features, replace=False):\n",
    "                cur_left = ind[X[ind, k] < 0.5]\n",
    "                cur_right = ind[X[ind, k] > 0.5]\n",
    "                if (len(cur_left) >= self.min_samples_leaf and len(cur_right)>= self.min_samples_leaf) or len(cur_right) == 0:\n",
    "                    cur_gain = gain(y[cur_left], y[cur_right], self.criterion)\n",
    "                    if best_gain is None or cur_gain > best_gain:\n",
    "                        best_f = k\n",
    "                        best_left = cur_left\n",
    "                        best_right = cur_right\n",
    "                        best_gain = cur_gain\n",
    "            if best_right is None or len(best_right) == 0:\n",
    "                return make_leaf()\n",
    "            else:\n",
    "                right_son = self.make_node(best_right, X, y, depth+1)\n",
    "                left_son = self.make_node(best_left, X, y, depth+1)\n",
    "                cur_node = DecisionTreeNode(best_f, best_s, left_son, right_son)\n",
    "                return cur_node\n",
    "                \n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        self.root = self.make_node(np.arange(X.shape[0]), X, y, 0)\n",
    "    \n",
    "    def query(self, x):\n",
    "        cur_node = self.root\n",
    "        while not isinstance(cur_node, DecisionTreeLeaf):\n",
    "            if x[cur_node.split_dim]  <= cur_node.split_value:\n",
    "                cur_node = cur_node.left\n",
    "            else:\n",
    "                cur_node = cur_node.right\n",
    "        return cur_node.probs\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        ans = []\n",
    "        for i in range(X.shape[0]):\n",
    "            ans.append(self.query(X[i]))\n",
    "        return ans\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return [max(p.keys(), key=lambda k: p[k]) for p in proba]\n",
    "    \n",
    "    def error(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        err = len([1 for i in zip(y_pred, y_test) if i[0] != i[1]])\n",
    "        return err/len(y_test)\n",
    "    \n",
    "    def feature_importance(self):\n",
    "        err = self.error(self.Xoob, self.yoob)\n",
    "        res = np.zeros(self.X.shape[1])\n",
    "        for i in range(self.X.shape[1]):\n",
    "            sX = self.Xoob.copy()\n",
    "            np.random.shuffle(sX[:,i])\n",
    "            res[i] = self.error(sX, self.yoob) - err\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 (2 балла)\n",
    "Теперь реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание.\n",
    "\n",
    "#### Параметры конструктора\n",
    "`n_estimators` - количество используемых для предсказания деревьев.\n",
    "\n",
    "Остальное - параметры деревьев.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n",
    "\n",
    "`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\", n_estimators=10):\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.forest = [None]*n_estimators\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.dtyp = y.dtype\n",
    "        self.features = X.shape[1]\n",
    "        for i in range(len(self.forest)):\n",
    "            self.forest[i] = DecisionTree(X, y, self.criterion, self.max_depth, self.min_samples_leaf, self.max_features)\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        res = np.zeros(shape=(X.shape[0], len(self.forest)), dtype=self.dtyp)\n",
    "        for i in range(len(self.forest)):\n",
    "            res[:,i] = self.forest[i].predict(X)\n",
    "        cls = []\n",
    "        for i in range(X.shape[0]):\n",
    "            cls.append(Counter(res[i]).most_common(1)[0][0])\n",
    "        return np.array(cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (2 балла)\n",
    "Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest - посчитать out-of-bag ошибку предсказания `err_oob`, а затем перемешать значения признака `j` и посчитать ее (`err_oob_j`) еще раз. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n",
    "\n",
    "Реализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(rfc):\n",
    "    res = np.zeros(rfc.features)\n",
    "    for tree in rfc.forest:\n",
    "        res += tree.feature_importance()\n",
    "    return res / len(rfc.forest)\n",
    "\n",
    "def most_important_features(importance, names, k=20):\n",
    "    # Выводит названия k самых важных признаков\n",
    "    idicies = np.argsort(importance)[::-1][:k]\n",
    "    return np.array(names)[idicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, пришло время протестировать наше дерево на простом синтетическом наборе данных. В результате должна получиться точность `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Importance: [-1.23328780e-03 -3.46393375e-04  1.56848928e-01  1.66211011e-01\n",
      "  3.21926431e-01  2.83577569e-04]\n"
     ]
    }
   ],
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3, \n",
    "          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = synthetic_dataset(1000)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X, y)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X) == y))\n",
    "print(\"Importance:\", feature_importance(rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4 (1 балл)\n",
    "Теперь поработаем с реальными данными.\n",
    "\n",
    "Выборка состоит из публичных анонимизированных данных пользователей социальной сети Вконтакте. Первые два столбца отражают возрастную группу (`zoomer`, `doomer` и `boomer`) и пол (`female`, `male`). Все остальные столбцы являются бинарными признаками, каждый из них определяет, подписан ли пользователь на определенную группу/публичную страницу или нет.\n",
    "\n",
    "Необходимо обучить два классификатора, один из которых определяет возрастную группу, а второй - пол.\n",
    "\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются. Лес должен строиться за какое-то разумное время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    dataframe = pandas.read_csv(path, header=0)\n",
    "    dataset = dataframe.values.tolist()\n",
    "    random.shuffle(dataset)\n",
    "    y_age = [row[0] for row in dataset]\n",
    "    y_sex = [row[1] for row in dataset]\n",
    "    X = [row[2:] for row in dataset]\n",
    "    \n",
    "    return np.array(X), np.array(y_age), np.array(y_sex), list(dataframe.columns)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
    "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7238335435056746\n",
      "Most important features:\n",
      "1. ovsyanochan\n",
      "2. 4ch\n",
      "3. styd.pozor\n",
      "4. rhymes\n",
      "5. dayvinchik\n",
      "6. rapnewrap\n",
      "7. mudakoff\n",
      "8. iwantyou\n",
      "9. pixel_stickers\n",
      "10. memeboizz\n",
      "11. pravdashowtop\n",
      "12. leprum\n",
      "13. ne1party\n",
      "14. bot_maxim\n",
      "15. reflexia_our_feelings\n",
      "16. borsch\n",
      "17. soverwenstvo.decora\n",
      "18. thesmolny\n",
      "19. pozor\n",
      "20. onlyorly\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "rfc.fit(X_train, y_age_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_age_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8511979823455234\n",
      "Most important features:\n",
      "1. 40kg\n",
      "2. 4ch\n",
      "3. modnailru\n",
      "4. zerofat\n",
      "5. girlmeme\n",
      "6. mudakoff\n",
      "7. i_d_t\n",
      "8. femalemem\n",
      "9. cook_good\n",
      "10. 9o_6o_9o\n",
      "11. sh.cook\n",
      "12. rapnewrap\n",
      "13. fuck_humor\n",
      "14. be.beauty\n",
      "15. be.women\n",
      "16. psy.people\n",
      "17. igm\n",
      "18. thesmolny\n",
      "19. reflexia_our_feelings\n",
      "20. club52205838\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "rfc.fit(X_train, y_sex_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_sex_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost\n",
    "В качестве аьтернативы попробуем CatBoost. \n",
    "\n",
    "Устаниовить его можно просто с помощью `pip install catboost`. Туториалы можно найти, например, [здесь](https://catboost.ai/docs/concepts/python-usages-examples.html#multiclassification) и [здесь](https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb). Главное - не забудьте использовать `loss_function='MultiClass'`.\n",
    "\n",
    "Сначала протестируйте CatBoost на синтетических данных. Выведите точность и важность признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1a29418198>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatBoostClassifier(loss_function='MultiClass')\n",
    "X, y = synthetic_dataset(1000)\n",
    "train_dataset = Pool(data=X,\n",
    "                     label=y,\n",
    "                     cat_features=None)\n",
    "model.fit(train_dataset, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Importance: [1.24341533e-02 1.97110774e-03 2.78396701e+01 2.78290121e+01\n",
      " 4.43092689e+01 7.64369948e-03]\n"
     ]
    }
   ],
   "source": [
    "X, y = synthetic_dataset(1000)\n",
    "\n",
    "print(\"Accuracy:\", np.mean(model.predict(train_dataset).reshape(y.shape) == y))\n",
    "print(\"Importance:\", model.get_feature_importance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5 (3 балла)\n",
    "Попробуем применить один из используемых на практике алгоритмов. В этом нам поможет CatBoost. Также, как и реализованный ними RandomForest, применим его для определения пола и возраста пользователей сети Вконтакте, выведите названия наиболее важных признаков так же, как в задании 3.\n",
    "\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
    "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)\n",
    "X_train, X_eval, y_age_train, y_age_eval, y_sex_train, y_sex_eval = train_test_split(X_train, y_age_train, y_sex_train, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7061790668348046\n",
      "Most important features:\n",
      "1. ovsyanochan\n",
      "2. styd.pozor\n",
      "3. mudakoff\n",
      "4. 4ch\n",
      "5. dayvinchik\n",
      "6. rhymes\n",
      "7. leprum\n",
      "8. xfilm\n",
      "9. rapnewrap\n",
      "10. iwantyou\n",
      "11. in.humour\n",
      "12. tumblr_vacuum\n",
      "13. fuck_humor\n",
      "14. bot_maxim\n",
      "15. dzenpub\n",
      "16. i_des\n",
      "17. pixel_stickers\n",
      "18. bestad\n",
      "19. thesmolny\n",
      "20. bon\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(loss_function='MultiClass')\n",
    "train_dataset = Pool(data=X_train,\n",
    "                     label=y_age_train,\n",
    "                     cat_features=None)\n",
    "test_dataset = Pool(data=X_test,\n",
    "                    label=y_age_test,\n",
    "                    cat_features=None)\n",
    "\n",
    "model.fit(train_dataset, eval_set=(X_eval, y_age_eval), silent=True)\n",
    "\n",
    "print(\"Accuracy:\", np.mean(model.predict(test_dataset).reshape(y_age_test.shape) == y_age_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(model.get_feature_importance(), features, 20)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8549810844892812\n",
      "Most important features:\n",
      "1. 40kg\n",
      "2. girlmeme\n",
      "3. modnailru\n",
      "4. mudakoff\n",
      "5. thesmolny\n",
      "6. femalemem\n",
      "7. igm\n",
      "8. 9o_6o_9o\n",
      "9. i_d_t\n",
      "10. be.beauty\n",
      "11. academyofman\n",
      "12. reflexia_our_feelings\n",
      "13. rapnewrap\n",
      "14. bon\n",
      "15. zerofat\n",
      "16. woman.blog\n",
      "17. sh.cook\n",
      "18. cook_good\n",
      "19. bot_maxim\n",
      "20. science_technology\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(loss_function='MultiClass')\n",
    "train_dataset = Pool(data=X_train,\n",
    "                     label=y_sex_train,\n",
    "                     cat_features=None)\n",
    "test_dataset = Pool(data=X_test,\n",
    "                    label=y_sex_test,\n",
    "                    cat_features=None)\n",
    "\n",
    "model.fit(train_dataset, eval_set=(X_eval, y_sex_eval), silent=True)\n",
    "\n",
    "print(\"Accuracy:\", np.mean(model.predict(test_dataset).reshape(y_sex_test.shape) == y_sex_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(model.get_feature_importance(), features, 20)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
