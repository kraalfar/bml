{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекуррентные сети пишут тексты\n",
    "__Суммарное количество баллов: 12__\n",
    "\n",
    "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
    "\n",
    "__Тема письма: `[ML][MS][HW06] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n",
    "\n",
    "В этом задании вам предстоит познакомиться с объединением Deep Learning и NLP. Для начала предстоит построить векторное пространство для словоря, а затем применить его для предсказания следующих слов в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.autograd import Variable\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = json.load(open(\"opencorp.json\", \"r\", encoding=\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 (3 балла)\n",
    "Чтобы обучить нейронную сеть, нам нужен датасет. В данном задании предлагается использовать данные, полученные из корпуса текстов OpenCorpora. Более того, датасет нужно представить в удобном виде. Поскольку мы хотим обучать эмбеддинги на парах `(token_center, token_context)`, а также иметь возможность делать `negative sampling`, датасет должен уметь выдавать соответствующие пары, а так же `negative sampling`-токены. \n",
    "\n",
    "Кроме того, мы бы не хотели строить эмбеддинги для очень редких слов, поэтому в словарь и в пары должны входить только слова, которые встречаются более `count_threshold` раз, а остальные должны быть заменены на специальный токен `\"<UNKNOWN>\"`. Последовательность должна начинаться с токена `\"<START>\"` и заканчиваться токеном `\"<END>\"`.\n",
    "\n",
    "#### Методы\n",
    "`__init__` - принимает на вход список последовтельностей токенов, преобразуя в соответствии с описанными выше критериями. При инициализации списка токенов нужно учитывать, что с вероятностью $1 - (\\sqrt{\\frac{0.001}{f(t)}} + 1) \\cdot \\frac{f(t)}{0.001}$ ($f(t)$ - частота слова в корпусе) мы \"выкидываем\" слово из текста, не добавляя никакие пары токенов с его участием в список. Это нужно для того, чтобы мы не переобучались на часто встречаемые слова. Также в `self.voc` должен записать актуальный словарь токенов.\n",
    "\n",
    "`__len__` - возвращает количество пар `(token_center, token_context)`\n",
    "\n",
    "`__getitem__` - принимает на вход индекс `i`, соответствующий паре `(t_i, c_i)`. Возвращает пару тензоров `(t_i, [c_i] + negatives)`, где `negatives` - список негативных токенов.\n",
    "\n",
    "`negative_sampling` - осуществляет взвешенное негативное сэмплирование. Вес токена определяется как $\\frac{(count(t))^{0.75}}{\\sum (count(t))^{0.75}}$, т.е. в negative samples частые слова попадают чаще, чем другие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokenized_sources, window=3, count_threshold=5, negative_sampling=5):\n",
    "        self.token_pairs = []\n",
    "        self.neg_samp = negative_sampling\n",
    "        self.counts = {\"<START>\": len(tokenized_sources), \"<END>\": len(tokenized_sources), \"<UNKNOWN>\":0}\n",
    "        \n",
    "        total_words_count = 0\n",
    "        for sent in tokenized_sources:\n",
    "            total_words_count += len(sent)\n",
    "            for word in sent:\n",
    "                if word in self.counts:\n",
    "                    self.counts[word] += 1\n",
    "                else:\n",
    "                    self.counts[word] = 1\n",
    "        \n",
    "        rem_words = []\n",
    "        for word in self.counts:\n",
    "            if self.counts[word] < count_threshold:\n",
    "                if word not in [\"<UNKNOWN>\", \"<START>\", \"<END>\"]:\n",
    "                    self.counts[\"<UNKNOWN>\"] += self.counts[word]\n",
    "                    rem_words.append(word)\n",
    "        [self.counts.pop(word) for word in rem_words]\n",
    "\n",
    "        to_rem = copy.deepcopy(tokenized_sources)\n",
    "        for sent in to_rem:\n",
    "            for i  in range(len(sent)):\n",
    "                if sent[i] in self.counts:\n",
    "                    word = sent[i]\n",
    "                else:\n",
    "                    word = \"<UNKNOWN>\"\n",
    "\n",
    "                if word not in [\"<START>\", \"<END>\"]:\n",
    "                    f = self.counts[word] / total_words_count * 1000\n",
    "                    f = 1 / f\n",
    "                    if np.random.rand() < 1 - np.sqrt(f) - f:\n",
    "                        sent[i] = \"<DELETED>\"\n",
    "        \n",
    "        for sent in tokenized_sources:\n",
    "            ns = [\"<START>\"] + sent + [\"<END>\"]\n",
    "            for i, word in enumerate(ns):\n",
    "                if word in self.counts:\n",
    "                    word_to_add = word\n",
    "\n",
    "                    if word == \"<DELETED>\":\n",
    "                        continue\n",
    "\n",
    "                    if word not in self.counts:\n",
    "                        word_to_add = \"<UNKNOWN>\"\n",
    "\n",
    "                    for j in range(-window, window+1):\n",
    "                        pos = i + j\n",
    "                        if pos >= 0 and pos < len(ns) and ns[pos] in self.counts and pos != i and ns[pos] != \"<DELETED>\":\n",
    "                            if self.counts[ns[pos]] < count_threshold:\n",
    "                                self.token_pairs.append((word_to_add, \"<UNKNOWN>\"))\n",
    "                            else:\n",
    "                                self.token_pairs.append((word_to_add, ns[pos]))\n",
    "            \n",
    "        \n",
    "\n",
    "        self.voc = np.array(list(self.counts.keys()))\n",
    "        self.token2id = dict([(token, i) for i, token in enumerate(self.voc)])\n",
    "                    \n",
    "    def negative_sampling(self):\n",
    "        den = 0\n",
    "        for word in self.counts:\n",
    "            den += pow(self.counts[word], 0.75)\n",
    "        ind = np.random.choice(range(len(self.counts)), \n",
    "                               p=[pow(self.counts[word], 0.75) / den for word in self.voc], \n",
    "                               size=self.neg_samp, \n",
    "                               replace=False)\n",
    "        return self.voc[ind]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ns = [self.token2id[i] for i in self.negative_sampling()];\n",
    "        t, c = self.token_pairs[index]\n",
    "        if t not in self.voc:\n",
    "            t = \"<UNKNOWN>\"\n",
    "        if c not in self.voc:\n",
    "            c = \"<UNKNOWN>\"\n",
    "        \n",
    "        t = self.token2id[t]\n",
    "        c = self.token2id[c]\n",
    "        \n",
    "\n",
    "        return torch.tensor([t]), torch.tensor([c] + ns, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor(dataset.negative_sampling())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640.4794921875"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TokenDataset(tokenized, count_threshold=20, window=2, negative_sampling=10)\n",
    "len(dataset) / 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 (4 балла)\n",
    "Теперь реализуем непосредственно SkipGram. Для этого нам потребуются `torch.autograd.Variables` чтобы сделать эмбеддинги обучаемыми. Также сразу реализуем интерфейс, которым будем пользоваться для применения эмбеддингов в следующих задачах.\n",
    "\n",
    "#### Методы SkipGram\n",
    "`__init__` - принимает на вход словарь и размерность пространств эмбеддингов. Инициализирует эмбеддинги для центрального и контекстного слов.\n",
    "\n",
    "`get_variables` - возвращает лист из всех `torch.autograd.Variables`. Необходимо, чтобы инициализировать оптимизатор.\n",
    "\n",
    "`predict_proba(center_tokens, context_tokens)` - принимает на вход список центральных токенов и список списков токенов из предполагаемого контекста. Для каждого центрального токена и соответствующего ему списка контекстных токенов должен вернуть скалярное произведение центрального и контекстуального эмбеддингов.\n",
    "\n",
    "#### Методы Embedding\n",
    "`__init__` - принимает на вход обученный SkipGram\n",
    "\n",
    "`embed` - возвращает эмбеддинги для всех элементов списка\n",
    "\n",
    "`reconstruct` - для всех элементов списка возвращает наиболее подходящий токен. Не возвращает `\"<UNKNOWN>\"`\n",
    "\n",
    "`n_closest` - возвращает `n` ближайших токенов для каждого элемента списка. Не возвращает `\"<UNKNOWN>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram:\n",
    "    def __init__(self, voc, latent_size):\n",
    "        self.id2token = list(voc)\n",
    "        self.token2id = dict([(token, i) for i, token in enumerate(self.id2token)])\n",
    "        self.v = Variable(torch.randn(len(voc) ,latent_size).float(), requires_grad=True)\n",
    "        self.u = Variable(torch.randn(latent_size, len(voc)).float(), requires_grad=True)\n",
    "        self.ls = latent_size\n",
    "        \n",
    "    def get_variables(self):\n",
    "        return [self.v, self.u]\n",
    "    \n",
    "    def predict_proba(self, center_ids, context_ids):\n",
    "        res = torch.zeros(len(center_ids), len(context_ids[0])).cuda()\n",
    "        for i, cid in enumerate(center_ids):\n",
    "            res[i] = torch.matmul(self.v[cid], self.u[:,context_ids[i]])\n",
    "        return res\n",
    "            \n",
    "class Embedding:\n",
    "    def __init__(self, skip_gram):\n",
    "        self.skip_gram = skip_gram\n",
    "        self.tree = KDTree(skip_gram.v.detach().numpy())\n",
    "    \n",
    "    def embed(self, tokens):\n",
    "        res = torch.zeros(len(tokens) , self.skip_gram.ls)\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in self.skip_gram.token2id:\n",
    "                res[i] = self.skip_gram.v[self.skip_gram.token2id[token]]\n",
    "            else:\n",
    "                res[i] = self.skip_gram.v[self.skip_gram.token2id[\"<UNKNOWN>\"]]\n",
    "        return res.detach().numpy()\n",
    "    \n",
    "    def reconstruct(self, embeddings):\n",
    "        _, inds = self.tree.query(embeddings, 2)\n",
    "        res = []\n",
    "        for ind in inds:\n",
    "            if not self.skip_gram.id2token[ind[0]] == \"<UNKNOWN>\":\n",
    "                res.append(self.skip_gram.id2token[ind[0]])\n",
    "            else:\n",
    "                res.append(self.skip_gram.id2token[ind[1]])\n",
    "        return res\n",
    "\n",
    "    def n_closest(self, embeddings, n=5):\n",
    "        res = [[] for _ in range(len(embeddings))]\n",
    "        _, inds = self.tree.query(embeddings, k=n+1)\n",
    "        for i, _ in enumerate(embeddings):\n",
    "            for ind in inds[i]:\n",
    "                if self.skip_gram.id2token[ind] != \"<UNKNOWN>\" and len(res[i]) < n:\n",
    "                    res[i].append(self.skip_gram.id2token[ind])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram = SkipGram(dataset.voc, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "Creating skipgram...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-28cbf54d95ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mskipgram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSkipGram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m             raise RuntimeError(\n\u001b[1;32m    191\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4096\n",
    "EPOCHS = 5\n",
    "\n",
    "print(\"Creating dataset...\")\n",
    "dataset = TokenDataset(tokenized, count_threshold=20, window=2, negative_sampling=5)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(\"Creating skipgram...\")\n",
    "skipgram = SkipGram(dataset.voc, 20)\n",
    "optim = torch.optim.Adam(skipgram.get_variables(), lr=1e-3)\n",
    "y = torch.tensor([0] * (BATCH_SIZE + 1), dtype=torch.long).cuda()\n",
    "losses = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    avg_loss = 0\n",
    "    steps = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        probs = skipgram.predict_proba(batch[0], batch[1])\n",
    "        loss = F.cross_entropy(probs, y[:len(probs)])\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        avg_loss += loss.item()\n",
    "        steps += 1\n",
    "    losses.append(avg_loss/steps)\n",
    "    print(\"Loss:\", avg_loss/steps)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(list(range(EPOCHS)), losses)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedding(skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.        , 14.79431252, 14.8442468 , 15.00804051],\n",
       "        [ 0.        , 14.79431252, 14.8442468 , 15.00804051]]),\n",
       " array([[   2,   78, 1323,   69],\n",
       "        [   2,   78, 1323,   69]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.tree.query([king, cat], k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['года', 'физического', 'же', 'неоднократно', 'увеличить', 'учений', 'твоя', 'предположения', 'следы', 'бы']\n",
      "['года', 'физического', 'же', 'неоднократно', 'увеличить', 'учений', 'твоя', 'предположения', 'следы', 'бы']\n",
      "['года', 'физического', 'же', 'неоднократно', 'увеличить', 'учений', 'твоя', 'предположения', 'следы', 'бы']\n",
      "['года', 'физического', 'же', 'неоднократно', 'увеличить', 'учений', 'твоя', 'предположения', 'следы', 'бы']\n",
      "['ты', 'белое', 'англии', 'коммерсантъ', 'расти', 'захочет', 'обычно', 'глядя', 'пункта', 'фонды']\n",
      "['я', 'преподаватель', 'нужен', 'произошло', 'выступая', 'китае', 'местных', 'сделана', 'свидетельствуют', 'социальных']\n"
     ]
    }
   ],
   "source": [
    "king = embedder.embed([\"король\"])[0]\n",
    "cat = embedder.embed([\"кошка\"])[0]\n",
    "owl = embedder.embed([\"сыч\"])[0]\n",
    "give = embedder.embed([\"дать\"])[0]\n",
    "me = embedder.embed([\"ты\"])[0]\n",
    "you = embedder.embed([\"я\"])[0]\n",
    "print(embedder.n_closest([king], 10)[0])\n",
    "print(embedder.n_closest([cat], 10)[0])\n",
    "print(embedder.n_closest([owl], 10)[0])\n",
    "print(embedder.n_closest([give], 10)[0])\n",
    "print(embedder.n_closest([me], 10)[0])\n",
    "print(embedder.n_closest([you], 10)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (2 балла)\n",
    "Теперь будем учиться восстанавливать слова в тексте. Для этого нам потребуется также определить датасет последовательностей фиксированной длинны.\n",
    "\n",
    "#### Методы\n",
    "`__init__` - принимает на вход `embedder` (обученный SkipGram) и список токенизированных последоватлеьностей `tokenized`.\n",
    "\n",
    "`__getitem__` - возвращает случайную закодированную при помощи SkipGram подпоследовательность длины `seq_len` одной из исходных последовательностей, сдвинутую на один токен подпоследовательность (т.е. следующие слова в тексте) и маску, которая отражает то, является ли токен неизвестным (`\"<UNKNOWN\"`).\n",
    "\n",
    "`__len__` - равна количеству последовательностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenSeqDataset(Dataset):\n",
    "    def __init__(self, embedder, tokenized, seq_len=32):\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenized = tokenized\n",
    "        self.embedder = embedder\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(tokenized)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if len(self.tokenized[index]) > self.seq_len:\n",
    "            top = len(self.tokenized[index]) - self.seq_len\n",
    "        else:\n",
    "            top = len(self.tokenized[index])\n",
    "            \n",
    "        ind = np.random.randint(0, top)\n",
    "        mask = self.tokenized[index][ind] in self.embedder.skip_gram.token2id\n",
    "        \n",
    "        seq = self.tokenized[ind: ind + self.seq_len + 1]\n",
    "        \n",
    "        resseq = self.embedder.embed[seq]\n",
    "        \n",
    "        return resseq[:-1], res[1:], mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4 (2 балла)\n",
    "Теперь обучим рекуррентную сеть, которая будет предсказывать следующее слово в тексте. Модель будет состоять из трех блоков: `input` (отвечает за предоброботку эмбеддинга), `rnn` (рекуррентная часть), `output` (отвечает за постобработку выхода).\n",
    "\n",
    "#### Методы\n",
    "`predict_sequential` - возвращает последовательность предсказаний для батча последовательностей\n",
    "\n",
    "`get_next` - предсказывает следующее слово\n",
    "\n",
    "`reset` - обнуляет внутреннее состояние сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN:\n",
    "    def __init__(self, latent_space=150, hidden_layer=512):\n",
    "        self.input = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_space, hidden_layer),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layer, hidden_layer),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.rnn = torch.nn.LSTM(hidden_layer, hidden_layer)\n",
    "        self.output = torch.nn.Linear(hidden_layer, latent_space)\n",
    "        self.input.cuda()\n",
    "        self.rnn.cuda()\n",
    "        self.output.cuda()\n",
    "        self.hidden = None\n",
    "    \n",
    "    def predict_sequential(self, sequences):\n",
    "        x, _ = self.rnn(self.input(sequences))\n",
    "        return self.output(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return list(self.input.parameters()) + list(self.rnn.parameters()) + list(self.output.parameters())\n",
    "    \n",
    "    def get_next(self, batch):\n",
    "        x, self.hidden = self.rnn(self.input(sequences).unsqueeze(1))\n",
    "        return self.output(x)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.hidden = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.nn.modules.rnn' from '/Users/faridbag/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py'>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.modules.rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-1166873761ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenSeqDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-e24ca463e813>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, latent_space, hidden_layer)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \"\"\"\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \"\"\"\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m             raise RuntimeError(\n\u001b[1;32m    191\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200\n",
    "\n",
    "dataset = TokenSeqDataset(embedder, tokenized)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "rnn = TextRNN(150)\n",
    "optim = torch.optim.Adam(rnn.parameters(), lr=1e-3)\n",
    "losses = []\n",
    "top1accs = []\n",
    "top5accs = []\n",
    "for i in range(EPOCHS):\n",
    "    avg_loss = 0\n",
    "    top1acc = 0\n",
    "    top5acc = 0\n",
    "    steps = 0\n",
    "    acc_steps = 0\n",
    "    for x, y_true, loss_mask in tqdm(dataloader):\n",
    "        y_pred = rnn.predict_sequential(x.cuda())\n",
    "        loss = (((y_true.cuda() - y_pred)**2).mean(dim=-1) * loss_mask.cuda()).mean()\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        avg_loss += loss.item()\n",
    "        steps += 1\n",
    "        if steps % 100 == 0:\n",
    "            acc_steps += 1\n",
    "            word_pred = embedder.n_closest(y_pred.detach().view(-1, 150)[:200].cpu().numpy())\n",
    "            word_true = embedder.reconstruct(y_true.detach().view(-1, 150)[:200].cpu().numpy())\n",
    "            unknown_mask = loss_mask.view(-1).cpu().numpy()\n",
    "            t1a = 0\n",
    "            t5a = 0\n",
    "            for true, pred, is_unknown in zip(word_true, word_pred, loss_mask):\n",
    "                if is_unknown:\n",
    "                    continue\n",
    "                if true == pred[0]:\n",
    "                    t1a += 1\n",
    "                if true in pred:\n",
    "                    t5a += 1\n",
    "            top1acc += t1a / len(word_pred)\n",
    "            top5acc += t5a / len(word_pred)\n",
    "    losses.append(avg_loss/steps)\n",
    "    top1accs.append(top1acc/acc_steps)\n",
    "    top5accs.append(top5acc/acc_steps)\n",
    "    print(\"Loss:\", avg_loss/steps)\n",
    "    print(\"Top-1 accuracy:\", top1acc/acc_steps)\n",
    "    print(\"Top-5 accuracy:\", top5acc/acc_steps)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(list(range(EPOCHS)), losses)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(list(range(100)), top1accs, label=\"Top-1\")\n",
    "plt.plot(list(range(100)), top5accs, label=\"Top-5\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5 (1 балл)\n",
    "Отлично, осталось только научитсья итеративно продолжать последовательность. Давайте попробуем научиться это делать.\n",
    "\n",
    "#### Методы\n",
    "`continue_sequence` - возвращает завершенную последовательность. Входная последовательность может быть пустой, поэтому в начало нужно добавить токен `\"<START>\"`. Закончить построение последовательности нужно после получения токена `\"<END>\"` или после получения `max_len` новых слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceCompleter:\n",
    "    def __init__(self, rnn, embedder, max_len=128):\n",
    "        self.rnn = rnn\n",
    "        self.embedder = embeder\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def continue_sequence(self, sequence):\n",
    "        t_sequence = [\"<START>\"] + sequence\n",
    "        embedding = self.embedder.embed(t_sequence)\n",
    "        self.rnn.reset()\n",
    "        with torch.no_grad():\n",
    "            for e in embedding:\n",
    "                x = self.rnn.get_next(torch.tensor([e], dtype=torch.float).cuda())\n",
    "            rec = self.embeder.reconstruct(x)\n",
    "            continued_sequence = []\n",
    "            ctn = 0\n",
    "            while rec[0] != \"<END>\" and ctn < self.max_len:\n",
    "                continued_sequence.append(rec[0])\n",
    "                e = self.embedder.embed(rec)\n",
    "                x = self.rnn.get_next(torch.tensor(e, dtype=torch.float).cuda())\n",
    "                rec = self.embeder.reconstruct(x)\n",
    "        return sequence + continued_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_completer = SequenceCompleter(rnn, embedder)\n",
    "print(seq_completer.continue_sequence([\"учеба\", \"в\", \"магистратуре\", \"-\", \"это\"]))\n",
    "print(seq_completer.continue_sequence([\"работает\", \"ли\", \"наша\", \"простая\", \"модель\", \"?\"]))\n",
    "print(seq_completer.continue_sequence([\"я\", \"точно\", \"знаю\"]))\n",
    "print(seq_completer.continue_sequence([\"машина\", \"времени\"]))\n",
    "print(seq_completer.continue_sequence([\"сегодня\"]))\n",
    "print(seq_completer.continue_sequence([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM:\n",
    "    def __init__(self, symbols):\n",
    "        self.hidden = None\n",
    "        self.input = torch.nn.Linear(len(symbols), 128)\n",
    "        self.lstm = torch.nn.LSTM(128, 128)\n",
    "        self.output = torch.nn.Sequential(torch.nn.Linear(128, 128), torch.nn.ReLU(), torch.nn.Linear(128, len(symbols)))\n",
    "    \n",
    "    def loss(self, batch):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
